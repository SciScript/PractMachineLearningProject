---
title: "Prediction of the Quality of Physical Activities"
output: html_document
---
## 1. Summary
The goal of this study is to predict how well people perform a certain physical activity based on data obtained from accelerometer the people wore while performing the activity. For this experiment, six participants performed ten repititions of the Unilateral Dumbbell Biceps Curl while wearing accelerometers, such as Jawbone Up, Nike FuelBand, and Fitbit, at their belt, forearm, arm, and dumbell. The lifts were performed correctly and incorrectly in five different ways: Class A) exactly according to the specification; Class B) throwing the elbows to the front; Class C) lifting the dumbbell only halfway; Class D) lowering the dumbbell only halfway; Class E) throwing the hips to the front. The obtained accelerometer data were stored at http://groupware.les.inf.puc-rio.br/har.

Based on these data, prediction models were built by identifying useful parameters for predicting the movement based on the given classification. These prediction models were finally used to predict the movement of twenty test cases.  

Training data were divided into two groups, a training data and a validation data (to be used to validate the data). The prediction model was derived from the training data. The model was validated by applying the criteria of an out-of-sample error rate of less than 0.5% or and accuracy of larger than 99.5%, which were required for a valide prediction of the twenty test cases.The training model using Random Forest was able to achieve an accuracy over 99.99%, or less than 0.03% out-of-sample error, and was able to predict the 20 test cases with an accuracy of 100%.

## 2. Analysis
```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

### Loading the Data
```{r}
set.seed(12345)

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```

```{r}
# Partitioning the training set
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

### Cleaning the Data
```{r}
# Remove columns with Near Zero Values
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]

# Remove the first column from the myTraining data set
myTraining <- myTraining[c(-1)]

# Remove variables with more than 60% NA
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   
        } 
    }
}

# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)

# Transform the myTesting and testing data sets
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the class column
myTesting <- myTesting[clean1]         # allow only variables in myTesting that are also in myTraining
testing <- testing[clean2]             # allow only variables in testing that are also in myTraining

# Coerce the data into the same type

for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}

# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]
```

### Decision Tree Prediction Model
```{r}
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
```

```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
```

```{r}
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix"))
```

The Decision Tree Prediction Model achieved an accuracy of `r round(cmtree$overall['Accuracy'], 4)`.

### Random Forest Prediction Model
```{r}
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
```

```{r}
plot(modFitB1, main="Fit of the Random Forest Prediction Model")
```

```{r}
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix"))
```

The Random Forest Prediction Model achieved an accuracy of `r round(cmrf$overall['Accuracy'], 4)`.

### Generlized Boosted Regression Prediction Model
```{r}
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)


gbmFinMod1 <- gbmFit1$finalModel

gbmPredTest <- predict(gbmFit1, newdata=myTesting)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, myTesting$classe)
gbmAccuracyTest
```

```{r}
plot(gbmFit1, ylim=c(0.9, 1))
```

### Prediction of the Test Data
Random Forests tested the test data with an accuracy of 99.89%. This accuracy was higher than those obtained with the models based on Decision Trees or Generalized Boosted Regression. The expected out-of-sample error was 0.11%.

```{r}
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2
```

## 3. Conclusion
The model predicted the twenty test cases with an accuracy of 100%. 
